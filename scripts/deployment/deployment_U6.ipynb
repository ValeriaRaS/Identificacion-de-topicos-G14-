{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "from pyngrok import ngrok\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "import joblib\n",
        "import numpy as np\n",
        "import uvicorn\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "gwiPu-Q3MDEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qnGHfrGLp89"
      },
      "outputs": [],
      "source": [
        "# Guarda los vectorizadores\n",
        "joblib.dump(vectorizer_todo, \"vectorizer_todo.pkl\")\n",
        "joblib.dump(vectorizer_bajo, \"vectorizer_bajo.pkl\")\n",
        "joblib.dump(vectorizer_alto, \"vectorizer_alto.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurar MLFlow para registrar modelos\n",
        "mlflow.set_tracking_uri(\"sqlite:///mlruns.db\")\n",
        "\n",
        "# Partiendo de los modelos entrenados: lda_todos, lda_altos, lda_bajos\n",
        "# Se registran en MLFlow\n",
        "with mlflow.start_run(run_name=\"lda_todos\"):\n",
        "    mlflow.sklearn.log_model(lda_todos, \"model\", registered_model_name=\"lda_todos\")\n",
        "\n",
        "with mlflow.start_run(run_name=\"lda_altos\"):\n",
        "    mlflow.sklearn.log_model(lda_altos, \"model\", registered_model_name=\"lda_altos\")\n",
        "\n",
        "with mlflow.start_run(run_name=\"lda_bajos\"):\n",
        "    mlflow.sklearn.log_model(lda_bajos, \"model\", registered_model_name=\"lda_bajos\")\n",
        "\n",
        "print(\"Modelos registrados en MLFlow\")"
      ],
      "metadata": {
        "id": "lxUPEGzJL3Yi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuración para Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Configurar URI de MLFlow\n",
        "mlflow.set_tracking_uri(\"sqlite:///mlruns.db\")\n",
        "\n",
        "# Cargar los vectorizadores desde los archivos\n",
        "vectorizer_todo_2 = joblib.load(\"vectorizer_todo.pkl\")\n",
        "vectorizer_bajo_2 = joblib.load(\"vectorizer_bajo.pkl\")\n",
        "vectorizer_alto_2 = joblib.load(\"vectorizer_alto.pkl\")\n",
        "\n",
        "\n",
        "# Inicializar FastAPI\n",
        "app = FastAPI()\n",
        "\n",
        "# Estructura para la solicitud\n",
        "class TopicRequest(BaseModel):\n",
        "    texto: str\n",
        "    modelo: str  # Puede ser \"todos\", \"altos\", \"bajos\"\n",
        "\n",
        "@app.post(\"/predict_topic/\")\n",
        "def predict_topic(request: TopicRequest):\n",
        "    \"\"\"\n",
        "    API que predice el tópico dominante usando un modelo de LDA seleccionado desde MLFlow.\n",
        "    \"\"\"\n",
        "    # Seleccionar el modelo y vectorizador\n",
        "    if request.modelo == \"todos\":\n",
        "        model_name = \"lda_todos\"\n",
        "        vectorizer = vectorizer_todo_2\n",
        "    elif request.modelo == \"altos\":\n",
        "        model_name = \"lda_altos\"\n",
        "        vectorizer = vectorizer_alto_2\n",
        "    elif request.modelo == \"bajos\":\n",
        "        model_name = \"lda_bajos\"\n",
        "        vectorizer = vectorizer_bajo_2\n",
        "    else:\n",
        "        raise HTTPException(status_code=400, detail=\"El modelo debe ser 'todos', 'altos' o 'bajos'.\")\n",
        "\n",
        "    # Cargar el modelo desde MLFlow\n",
        "    try:\n",
        "        model_uri = f\"models:/{model_name}/latest\"\n",
        "        lda_model = mlflow.sklearn.load_model(model_uri)\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=f\"No se pudo cargar el modelo {model_name}: {str(e)}\")\n",
        "\n",
        "    # Transformar el texto en Bag of Words\n",
        "    features = vectorizer.transform([request.texto])\n",
        "\n",
        "    # Verificar si hay términos válidos\n",
        "    if features.nnz == 0:\n",
        "        raise HTTPException(status_code=400, detail=\"El texto no contiene términos reconocidos por el modelo.\")\n",
        "\n",
        "    # Predicción del tópico\n",
        "    topic_distribution = lda_model.transform(features)\n",
        "    dominant_topic = np.argmax(topic_distribution)\n",
        "\n",
        "    # Respuesta\n",
        "    return {\n",
        "        \"modelo\": request.modelo,\n",
        "        \"tópico_dominante\": int(dominant_topic),\n",
        "        \"distribución_tópicos\": topic_distribution.tolist()\n",
        "    }\n",
        "\n",
        "# Inicia el túnel con ngrok\n",
        "public_url = ngrok.connect(8000)\n",
        "print(f\"Tu API está disponible en: {public_url}\")\n",
        "\n",
        "# Inicia el servidor FastAPI\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
      ],
      "metadata": {
        "id": "1hH_3EYgL__q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}